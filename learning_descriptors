from sklearn import svm, model_selection, neural_network, neighbors, metrics, cluster, preprocessing
from sklearn.metrics.pairwise import chi2_kernel
import numpy as np
import time
import os
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import matplotlib.colors as colors

path = "C:/Users/OrdiPaul/Documents/Mines Nancy/Projet/Projet3A_wastesorting/dataset_resized/"



def descriptors(path, test_proportion, lib="SURF"):
    """creation des descripteurs a partir des images"""

    print("--- extraction des descripteurs :")

    # creation d'un createur de descripteur 2D
    if lib == "SURF":
        extrac = cv2.xfeatures2d.SURF_create()
    elif lib == "SIFT":
        extrac = cv2.xfeatures2d.SIFT_create()
    else:
        raise ValueError('descriptor extractor not supported, try "SIFT" or "SURF" ')

    descriptors_train = []
    descriptors_test = []

    idx_descriptors_train = []
    Y_train = []
    idx_descriptors_test = []
    Y_test = []

    classe = 0
    # on parcourt toutes les images du fichier
    idx_start_test = 0
    idx_start_train = 0
    for directory in os.listdir(path)[:6]:
        classe += 1
        print("classe %d : %s" % (classe, directory))

        nb_img = len(os.listdir(path + directory))
        i = 0
        for file in os.listdir(path + directory):
            i+=1

            # ouverture de l'image et passage en niveau de gris
            img = plt.imread(path + directory + '/' + file)
            img = np.mean(img, -1)

            # on transforme l'image en array dtype='uint8'
            img = np.array(img).flatten()
            img_uint8 = np.uint8(img)

            # les keypoints (coordonnées des patches) et descripteurs
            kp, des = extrac.detectAndCompute(img_uint8, None)
            len_des = len(des)

            # on cree les descripteurs d'apprentissage et de test dans les proportions choisies
            if i < nb_img * (1-test_proportion):
                # creation des listes qui construiront la base d'apprentissage
                descriptors_train.append(des)
                idx_descriptors_train.append((idx_start_train, len_des))
                Y_train.append(classe)

                # index de départ de la prochaine image
                idx_start_test += len_des

            else:
                # creation des listes qui contruiront la base de test
                descriptors_test.append(des)
                idx_descriptors_test.append((idx_start_train,len_des))
                Y_test.append(classe)

                # index de départ de la prochaine image
                idx_start_test += len_des

    print("--- Done")

    return descriptors_train, descriptors_test, \
           idx_descriptors_train, Y_train, \
           idx_descriptors_test, Y_test


def train_generation(idx_descriptors_train, n_words, KMeans):
    """generation de la base d'apprentissage"""

    print("--- creation de la base d'entrainement :")

    X_train =[]
    # chaque image est associee à un codeword (liste des fréquences d'apparition des mots visuels)
    for idx_img in range(len(idx_descriptors_train)):
        # on initialise l'histogramme a 0
        words_histogram = np.zeros(n_words)

        # on ajoute ensuite les mots correspondants aux descripteurs
        start, length = idx_descriptors_train[idx_img]
        for idx_des in range(start, start+length):
            associated_word = KMeans.labels_[idx_des]
            words_histogram[associated_word] += 1/length # pour normaliser la somme a 1

        X_train.append(words_histogram)

    print("--- Done")

    return X_train


def test_generation(descriptors_test, idx_descriptors_test, n_words, KMeans):
    """generation de la base de test"""

    print("--- creation de la base de test :")

    X_test = []
    # chaque image est associée à un codeword (liste des fréquences d'apparition des mots visuels)
    for idx_img in range(len(idx_descriptors_test)):
        # on initialise l'histogramme a 0
        words_histogram = np.zeros(n_words)

        # on ajoute ensuite les mots correspondants aux descripteurs
        start, length = idx_descriptors_test[idx_img]

        des = descriptors_test[idx_img]
        labels = KMeans.predict(des)
        for idx_word in labels:
            words_histogram[idx_word] += 1 / length  # pour normaliser la somme a 1

        X_test.append(words_histogram)

    print("--- Done")

    return X_test


def learn_NN(X_train, X_test, Y_train, Y_test, n_neighbors=1, n_words=500):
    """entraine un modele Nearest Neighbors avec le dossier d'image considere"""

    # modele NN
    NN = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)
    NN.fit(X_train, Y_train)
    score = NN.score(X_test, Y_test)
    print("score NN %d %.3f , n_words = %d" %(n_neighbors, score, n_words))
    Y_test_pred = NN.predict(X_test)
    print(metrics.confusion_matrix(Y_test, Y_test_pred))
    #display_train(X_image_test, X_size_test, Y_test, Y_test_pred, "NN")

    return score


def learn_SVM(X_train, X_test, Y_train, Y_test, kernel='linear', C=1, gamma=False, n_words=500, print_result=False):

    # modele SVM
    if gamma != False:
        SVM = svm.SVC(kernel=kernel, C=C, gamma=gamma)
    else:
        SVM = svm.SVC(kernel=kernel, C=C)
    SVM.fit(X_train,Y_train)
    score = SVM.score(X_test, Y_test)

    if print_result:
        print("score SVM %s %.3f , n_words %d, C = %f" %(kernel, score, n_words, C))
        Y_test_pred=SVM.predict(X_test)
        print(metrics.confusion_matrix(Y_test,Y_test_pred))

    return score


def idea_model(path, test_proportion=0.1):
    """pour se faire une petite idee de comment ca se passe"""

    # creation des descripteurs
    des_train, des_test, idx_des_train, Y_train, idx_des_test, Y_test = descriptors(path, test_proportion)

    for n_words in [200, 500, 1000]:

        # clustering par unsupervised-learning, creation du vocabulaire
        KMeans = cluster.MiniBatchKMeans(n_clusters=n_words, init_size=max(500,5*n_words))  # utiliser MiniBatchKmeans plutôt que KMeans
        train_conc = np.concatenate(des_train)
        KMeans.fit(train_conc)

        # creation des jeux de test et d'entrainement
        X_train = train_generation(idx_des_train, n_words, KMeans)
        X_test = test_generation(des_test, idx_des_test, n_words, KMeans)

        for p in range(0,5):
            for kernel in ['linear', 'rbf', chi2_kernel]:
                C = 10**p
                learn_SVM(X_train, X_test, Y_train, Y_test, kernel, C=C, n_words=n_words, print_result=True)


idea_model(path)


def test_model_svm(path, n_words=200, C=1000, test_proportion=0.1):
    """pour se faire une petite idee de comment ca se passe"""

    # creation des descripteurs
    des_train, des_test, idx_des_train, Y_train, idx_des_test, Y_test = descriptors(path, test_proportion)

    # clustering par unsupervised-learning, creation du vocabulaire
    KMeans = cluster.MiniBatchKMeans(n_clusters=n_words, init_size=3*n_words)  # utiliser MiniBatchKmeans plutôt que KMeans
    train_conc = np.concatenate(des_train)
    KMeans.fit(train_conc)

    # creation des jeux de test et d'entrainement
    X_train = train_generation(idx_des_train, n_words, KMeans)
    X_test = test_generation(des_test, idx_des_test, n_words, KMeans)

    # for nn in range(1, 20, 3):
    #     learn_NN(X_train, X_test, Y_train, Y_test, n_neighbors=nn, n_words=n_words)

    for kernel in ['linear', 'rbf', chi2_kernel]:
        learn_SVM(X_train, X_test, Y_train, Y_test, kernel, C=C, n_words=n_words, print_result=True)

#test_model_svm(path, n_words=200, C=1000, test_proportion=0.1)

def test_model_ppv(path, n_words=200, test_proportion=0.1):
    """pour se faire une petite idee de comment ca se passe"""

    print("test_model_ppv :")

    # creation des descripteurs
    des_train, des_test, idx_des_train, Y_train, idx_des_test, Y_test = descriptors(path, test_proportion, lib="SIFT")

    for n_words in [20,50,100,300,1000]:

        # clustering par unsupervised-learning, creation du vocabulaire
        print("N_words = ",n_words)
        KMeans = cluster.MiniBatchKMeans(n_clusters=n_words, init_size=3*n_words)  # utiliser MiniBatchKmeans plutôt que KMeans
        train_conc = np.concatenate(des_train)
        KMeans.fit(train_conc)

        # creation des jeux de test et d'entrainement
        X_train = train_generation(idx_des_train, n_words, KMeans)
        X_test = test_generation(des_test, idx_des_test, n_words, KMeans)

        for nn in range(1, 20, 3):
            learn_NN(X_train, X_test, Y_train, Y_test, n_neighbors=nn, n_words=n_words)

#test_model_ppv(path)

def draw_parameters(path, C=1000, n_words=100, test_proportion=0.1):
    """try to find the best parameters"""

    # creation des descripteurs
    des_train, des_test, idx_des_train, Y_train, idx_des_test, Y_test = descriptors(path, test_proportion)

    # clustering par unsupervised-learning, creation du vocabulaire
    KMeans = cluster.MiniBatchKMeans(n_clusters=n_words)  # utiliser MiniBatchKmeans plutôt que KMeans
    train_conc = np.concatenate(des_train)
    KMeans.fit(train_conc)

    # creation des jeux de test et d'entrainement
    X_train = train_generation(idx_des_train, n_words, KMeans)
    X_test = test_generation(des_test, idx_des_test, n_words, KMeans)

    nb_fig = 0

    # C plots___________________________________________________________________________________________________________
    nb_fig += 1
    list_param =[]
    score_linear = []
    score_rbf = []
    score_chi2 = []
    for p in np.linspace(-6,7,50):
        C_test = 10**p
        list_param.append(C_test)
        score_linear.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel='linear', C=C_test, n_words=n_words))
        score_rbf.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel='rbf', C=C_test, n_words=n_words))
        score_chi2.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel=chi2_kernel, C=C_test, n_words=n_words))

    # plot of the result
    plt.figure(nb_fig)
    plt.plot(list_param, score_linear, 'b', marker='o', linewidth=3, label="linear")
    plt.plot(list_param, score_rbf, 'g',  marker='o', linewidth=3, label="rbf")
    plt.plot(list_param, score_chi2, 'r',  marker='o', linewidth=3, label="chi2")
    plt.title("score(C), n_words= %d, test_proportion= %f.3" %(n_words, test_proportion))
    plt.xscale('log')
    plt.xlabel("hyperparameter C")
    plt.ylabel("classification score")
    plt.legend()

    plt.show()

    # gamma plots_______________________________________________________________________________________________________
    nb_fig += 1
    list_param =[]
    score_linear = []
    score_rbf = []
    score_chi2 = []
    for p in np.linspace(-6, 7, 50):
        gamma_test = 10**p
        list_param.append(gamma_test)
        score_linear.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel='linear', C=C, gamma=gamma_test, n_words=n_words))
        score_rbf.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel='rbf', C=C, gamma=gamma_test, n_words=n_words))
        score_chi2.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel=chi2_kernel, C=C, gamma=gamma_test, n_words=n_words))

    # plot of the result
    plt.figure(nb_fig)
    plt.plot(list_param, score_linear, 'b', marker='o', linewidth=3, label="linear")
    plt.plot(list_param, score_rbf, 'g',  marker='o', linewidth=3, label="rbf")
    plt.plot(list_param, score_chi2, 'r',  marker='o', linewidth=3, label="chi2")
    plt.title("score(gamma), C= %.2f, n_words= %d, test_proportion= %.3f" %(C, n_words, test_proportion))
    plt.xscale('log')
    plt.xlabel("hyperparameter gamma")
    plt.ylabel("classification score")
    plt.legend()
    plt.show()

    # n_words plots_____________________________________________________________________________________________________
    nb_fig += 1
    list_param =[]
    score_linear = []
    score_rbf = []
    score_chi2 = []
    for p in range(1, 51, 5):
        n_words_test = int(10**(p/10))
        # clustering par unsupervised-learning, creation du vocabulaire
        KMeans = cluster.MiniBatchKMeans(n_clusters=n_words_test, init_size=3*n_words)  # utiliser MiniBatchKmeans plutôt que KMeans
        train_conc = np.concatenate(des_train)
        KMeans.fit(train_conc)

        # creation des jeux de test et d'entrainement
        X_train = train_generation(idx_des_train, n_words_test, KMeans)
        X_test = test_generation(des_test, idx_des_test, n_words_test, KMeans)

        list_param.append(n_words_test)
        score_linear.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel='linear', C=C, n_words=n_words_test))
        score_rbf.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel='rbf', C=C, n_words=n_words_test))
        score_chi2.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel=chi2_kernel, C=C, n_words=n_words_test))

    # plot of the result
    plt.figure(nb_fig)
    plt.plot(list_param, score_linear, 'b', marker='o', linewidth=3, label="linear")
    plt.plot(list_param, score_rbf, 'g',  marker='o', linewidth=3, label="rbf")
    plt.plot(list_param, score_chi2, 'r',  marker='o', linewidth=3, label="chi2")
    plt.title("score(gamma), C= %f.2, test_proportion= %f.3" %(C, test_proportion))
    plt.xlabel("number of words in the histogram")
    plt.ylabel("classification score")
    plt.legend()
    plt.show()

#draw_parameters(path)


def score_colormap(path, n_words=100, test_proportion=0.1):
    """try to find the best parameters gamma and C the same time
    I use here a color map representing the score"""
    print("### score_colormap")

    # creation des descripteurs
    des_train, des_test, idx_des_train, Y_train, idx_des_test, Y_test = descriptors(path, test_proportion, lib="SIFT")

    # clustering par unsupervised-learning, creation du vocabulaire
    KMeans = cluster.MiniBatchKMeans(n_clusters=n_words)  # utiliser MiniBatchKmeans plutôt que KMeans
    train_conc = np.concatenate(des_train)
    KMeans.fit(train_conc)

    # creation des jeux de test et d'entrainement
    X_train = train_generation(idx_des_train, n_words, KMeans)
    X_test = test_generation(des_test, idx_des_test, n_words, KMeans)

    list_C = []
    list_gamma = []
    score_linear = []
    score_rbf = []
    score_chi2 = []

    list_C = np.linspace(-5, 5, 51)
    list_gamma = np.linspace(-5, 5, 51)
    for p in list_C:
        C = 10 ** p

        S_lin = []
        S_rbf = []
        S_chi = []

        for q in list_gamma:
            print(p,q)
            gamma = 10 ** q

            S_lin.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel='linear', gamma=gamma, C=C, n_words=n_words))
            S_rbf.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel='rbf', gamma=gamma, C=C, n_words=n_words))
            S_chi.append(learn_SVM(X_train, X_test, Y_train, Y_test, kernel=chi2_kernel, gamma=gamma, C=C, n_words=n_words))

        score_linear.append(S_lin)
        score_rbf.append(S_rbf)
        score_chi2.append(S_chi)

    plt.subplot(2,2,1)
    plt.pcolor(list_C, list_gamma, np.transpose(score_linear), cmap='RdBu_r', vmin=0, vmax=1)
    plt.title("linear")
    plt.xlabel("log(C)")
    plt.ylabel("log(gamma)")

    plt.subplot(2,2,2)
    plt.pcolor(list_C, list_gamma, np.transpose(score_rbf), cmap='RdBu_r', vmin=0, vmax=1)
    plt.title("rbf")
    plt.xlabel("log(C)")
    plt.ylabel("log(gamma)")

    plt.subplot(2,2,3)
    plt.pcolor(list_C, list_gamma, np.transpose(score_chi2), cmap='RdBu_r', vmin=0, vmax=1)
    plt.title("chi2")
    plt.xlabel("log(C)")
    plt.ylabel("log(gamma)")
    plt.colorbar()

    plt.tight_layout()
    plt.show()

#score_colormap(path, n_words=50, test_proportion=0.1)

